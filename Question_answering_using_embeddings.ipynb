{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b0435cb",
   "metadata": {},
   "source": [
    "# Question answering using embeddings-based search\n",
    "\n",
    "GPT excels at answering questions, but only on topics it remembers from its training data.\n",
    "\n",
    "What should you do if you want GPT to answer questions about unfamiliar topics? E.g.,\n",
    "- Recent events after Sep 2021\n",
    "- Your non-public documents\n",
    "- Information from past conversations\n",
    "- etc.\n",
    "\n",
    "This notebook demonstrates a two-step Search-Ask method for enabling GPT to answer questions using a library of reference text.\n",
    "\n",
    "1. **Search:** search your library of text for relevant text sections\n",
    "2. **Ask:** insert the retrieved text sections into a message to GPT and ask it the question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6e01be1",
   "metadata": {},
   "source": [
    "## Why search is better than fine-tuning\n",
    "\n",
    "GPT can learn knowledge in two ways:\n",
    "\n",
    "- Via model weights (i.e., fine-tune the model on a training set)\n",
    "- Via model inputs (i.e., insert the knowledge into an input message)\n",
    "\n",
    "Although fine-tuning can feel like the more natural option—training on data is how GPT learned all of its other knowledge, after all—we generally do not recommend it as a way to teach the model knowledge. Fine-tuning is better suited to teaching specialized tasks or styles, and is less reliable for factual recall.\n",
    "\n",
    "As an analogy, model weights are like long-term memory. When you fine-tune a model, it's like studying for an exam a week away. When the exam arrives, the model may forget details, or misremember facts it never read.\n",
    "\n",
    "In contrast, message inputs are like short-term memory. When you insert knowledge into a message, it's like taking an exam with open notes. With notes in hand, the model is more likely to arrive at correct answers.\n",
    "\n",
    "One downside of text search relative to fine-tuning is that each model is limited by a maximum amount of text it can read at once:\n",
    "\n",
    "| Model           | Maximum text length       |\n",
    "|-----------------|---------------------------|\n",
    "| `gpt-3.5-turbo` | 4,096 tokens (~5 pages)   |\n",
    "| `gpt-4`         | 8,192 tokens (~10 pages)  |\n",
    "| `gpt-4-32k`     | 32,768 tokens (~40 pages) |\n",
    "\n",
    "Continuing the analogy, you can think of the model like a student who can only look at a few pages of notes at a time, despite potentially having shelves of textbooks to draw upon.\n",
    "\n",
    "Therefore, to build a system capable of drawing upon large quantities of text to answer questions, we recommend using a Search-Ask approach.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78fba1de",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Text can be searched in many ways. E.g.,\n",
    "\n",
    "- Lexical-based search\n",
    "- Graph-based search\n",
    "- Embedding-based search\n",
    "\n",
    "This example notebook uses embedding-based search. [Embeddings](https://platform.openai.com/docs/guides/embeddings) are simple to implement and work especially well with questions, as questions often don't lexically overlap with their answers.\n",
    "\n",
    "Consider embeddings-only search as a starting point for your own system. Better search systems might combine multiple search methods, along with features like popularity, recency, user history, redundancy with prior search results, click rate data, etc. Q&A retrieval performance may also be improved with techniques like [HyDE](https://arxiv.org/abs/2212.10496), in which questions are first transformed into hypothetical answers before being embedded. Similarly, GPT can also potentially improve search results by automatically transforming questions into sets of keywords or search terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4ca8276-e829-4cff-8905-47534e4b4d4e",
   "metadata": {},
   "source": [
    "## Full procedure\n",
    "\n",
    "Specifically, this notebook demonstrates the following procedure:\n",
    "\n",
    "1. Prepare search data (once per document)\n",
    "    1. Collect: We'll download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "    2. Chunk: Documents are split into short, mostly self-contained sections to be embedded\n",
    "    3. Embed: Each section is embedded with the OpenAI API\n",
    "    4. Store: Embeddings are saved (for large datasets, use a vector database)\n",
    "2. Search (once per query)\n",
    "    1. Given a user question, generate an embedding for the query from the OpenAI API\n",
    "    2. Using the embeddings, rank the text sections by relevance to the query\n",
    "3. Ask (once per query)\n",
    "    1. Insert the question and the most relevant sections into a message to GPT\n",
    "    2. Return GPT's answer\n",
    "\n",
    "### Costs\n",
    "\n",
    "Because GPT is more expensive than embeddings search, a system with a decent volume of queries will have its costs dominated by step 3.\n",
    "\n",
    "- For `gpt-3.5-turbo` using ~1,000 tokens per query, it costs ~$0.002 per query, or ~500 queries per dollar (as of Apr 2023)\n",
    "- For `gpt-4`, again assuming ~1,000 tokens per query, it costs ~$0.03 per query, or ~30 queries per dollar (as of Apr 2023)\n",
    "\n",
    "Of course, exact costs will depend on the system specifics and usage patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ebd41d8",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "We'll begin by:\n",
    "- Importing the necessary libraries\n",
    "- Selecting models for embeddings search and question answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3839a6-9146-4f60-b74b-19abbc24278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai  # for calling the OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fcace0f",
   "metadata": {},
   "source": [
    "#### Troubleshooting: Installing libraries\n",
    "\n",
    "If you need to install any of the libraries above, run `pip install {library_name}` in your terminal.\n",
    "\n",
    "For example, to install the `openai` library, run:\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai` or `%pip install openai`.)\n",
    "\n",
    "After installing, restart the notebook kernel so the libraries can be loaded.\n",
    "\n",
    "#### Troubleshooting: Setting your API key\n",
    "\n",
    "The OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, you can set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc2d8de",
   "metadata": {},
   "source": [
    "## 1. Prepare search data\n",
    "\n",
    "To save you the time & expense, we've prepared a pre-embedded dataset of a few hundred Wikipedia articles about the 2022 Winter Olympics.\n",
    "\n",
    "To see how we constructed this dataset, or to modify it yourself, see [Embedding Wikipedia articles for search](Embedding_Wikipedia_articles_for_search.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d50792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in pre-chunked text and pre-computed embeddings\n",
    "embeddings_path = \"data/phone_review_test.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70307f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert embeddings from CSV str type back to list type\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "424162c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>samsung-galaxy-s23-ultra\\nThe Samsung Galaxy S...</td>\n",
       "      <td>[0.0019498983165249228, 0.01320930290967226, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>samsung-galaxy-s23-ultra\\nSo what can you do w...</td>\n",
       "      <td>[0.006145430728793144, 0.02437526173889637, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>samsung-galaxy-s23-ultra\\nKeep an eye on Galax...</td>\n",
       "      <td>[-0.0005396574269980192, -0.009514674544334412...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samsung-galaxy-s23-ultra\\nOther Galaxy S23 Ult...</td>\n",
       "      <td>[1.849132422648836e-05, 0.012831293977797031, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samsung-galaxy-s23-ultra\\nBased on my extensiv...</td>\n",
       "      <td>[0.004560495261102915, 0.003177931299433112, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>iphone-13-pro-max\\nThe iPhone 13 Pro Max is th...</td>\n",
       "      <td>[-0.00946941040456295, 0.0015022949082776904, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>iphone-13-pro-max\\nI also appreciate the sligh...</td>\n",
       "      <td>[-0.0158053208142519, -0.0011863635154441, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>iphone-13-pro-max\\nSome may simply prefer the ...</td>\n",
       "      <td>[-0.014161327853798866, -0.0053186253644526005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>iphone-13-pro-max\\nOf course, you might want t...</td>\n",
       "      <td>[-0.010059180669486523, -0.01733901910483837, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>iphone-13-pro-max\\nNext: The Samsung Galaxy Z ...</td>\n",
       "      <td>[-0.012104046531021595, -0.004899413324892521,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    samsung-galaxy-s23-ultra\\nThe Samsung Galaxy S...   \n",
       "1    samsung-galaxy-s23-ultra\\nSo what can you do w...   \n",
       "2    samsung-galaxy-s23-ultra\\nKeep an eye on Galax...   \n",
       "3    samsung-galaxy-s23-ultra\\nOther Galaxy S23 Ult...   \n",
       "4    samsung-galaxy-s23-ultra\\nBased on my extensiv...   \n",
       "..                                                 ...   \n",
       "415  iphone-13-pro-max\\nThe iPhone 13 Pro Max is th...   \n",
       "416  iphone-13-pro-max\\nI also appreciate the sligh...   \n",
       "417  iphone-13-pro-max\\nSome may simply prefer the ...   \n",
       "418  iphone-13-pro-max\\nOf course, you might want t...   \n",
       "419  iphone-13-pro-max\\nNext: The Samsung Galaxy Z ...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [0.0019498983165249228, 0.01320930290967226, 0...  \n",
       "1    [0.006145430728793144, 0.02437526173889637, 0....  \n",
       "2    [-0.0005396574269980192, -0.009514674544334412...  \n",
       "3    [1.849132422648836e-05, 0.012831293977797031, ...  \n",
       "4    [0.004560495261102915, 0.003177931299433112, 0...  \n",
       "..                                                 ...  \n",
       "415  [-0.00946941040456295, 0.0015022949082776904, ...  \n",
       "416  [-0.0158053208142519, -0.0011863635154441, -0....  \n",
       "417  [-0.014161327853798866, -0.0053186253644526005...  \n",
       "418  [-0.010059180669486523, -0.01733901910483837, ...  \n",
       "419  [-0.012104046531021595, -0.004899413324892521,...  \n",
       "\n",
       "[420 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataframe has two columns: \"text\" and \"embedding\"\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec1c344c",
   "metadata": {},
   "source": [
    "## 2. Search\n",
    "\n",
    "Now we'll define a search function that:\n",
    "- Takes a user query and a dataframe with text & embedding columns\n",
    "- Embeds the user query with the OpenAI API\n",
    "- Uses distance between query embedding and text embeddings to rank the texts\n",
    "- Returns two lists:\n",
    "    - The top N texts, ranked by relevance\n",
    "    - Their corresponding relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a8c713-c8a9-47dc-85a4-871ee1395566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0efa0f6-4469-457a-89a4-a2f5736a01e0",
   "metadata": {},
   "source": [
    "## 3. Ask\n",
    "\n",
    "With the search function above, we can now automatically retrieve relevant knowledge and insert it into messages to GPT.\n",
    "\n",
    "Below, we define a function `ask` that:\n",
    "- Takes a user query\n",
    "- Searches for text relevant to the query\n",
    "- Stuffs that text into a message for GPT\n",
    "- Sends the message to GPT\n",
    "- Returns GPT's answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f45cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below articles on smart phone reviews to answer the subsequent question and give recommendation, \\\n",
    "        but do not write \"Based on the articles\" in the answer.\\\n",
    "        If the answer cannot be found in the articles, write \"I could not find such product\". \\\n",
    "        If there are multiple possible answers, give the answers and then write \"could you provide more information to narrow down the recommendation\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nphone review section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions and give recommendation about smart phone.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response_message\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f2b0927",
   "metadata": {},
   "source": [
    "### Example questions\n",
    "\n",
    "Finally, let's ask our system our original question about gold medal curlers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e11f53ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The latest released smart phone is the iPhone 14 Pro Max, which was released on September 16th.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('what is the latest released smart phone?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
